name: PyArrow Compatibility Tests

# This workflow tests compatibility with pyarrow 21, 22, and 23 across different
# Python versions, operating systems, and installation methods (pip vs conda).
# Reference: commit 2e5288e pinned pyarrow <22 due to ABI incompatibility concerns.
# This workflow validates that newer versions work correctly with both pip and conda builds.

on:
  push:
    branches: [ main, refactor ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly on Monday at 3 AM UTC to catch any new issues
    - cron: '0 3 * * 1'

jobs:
  test-pyarrow-pip:
    name: PyArrow ${{ matrix.pyarrow-version }} - Python ${{ matrix.python-version }} - ${{ matrix.os }} (pip)
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.10', '3.11']
        pyarrow-version: ['21.0.0', '22.0.0', '23.0.0']
    
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Upgrade pip, setuptools, and wheel
        run: |
          python -m pip install --upgrade pip setuptools wheel

      - name: Install pyarrow ${{ matrix.pyarrow-version }}
        run: |
          pip install "pyarrow==${{ matrix.pyarrow-version }}"

      - name: Install package with test dependencies
        run: |
          pip install -e .[dev]

      - name: Verify pyarrow version
        run: |
          python -c "import pyarrow; print(f'PyArrow version: {pyarrow.__version__}')"

      - name: Run pytest
        run: |
          pytest -v --tb=short

      - name: Run basic smoke test (read/write GeoParquet)
        run: |
          python -c "
          import tempfile
          import os
          import pyarrow as pa
          import pyarrow.parquet as pq
          from pathlib import Path
          
          # Create a simple test parquet file with geo metadata
          table = pa.table({
              'id': [1, 2, 3],
              'geometry': [b'POINT(0 0)', b'POINT(1 1)', b'POINT(2 2)']
          })
          
          # Add minimal GeoParquet metadata
          geo_metadata = {
              'version': '1.0.0',
              'primary_column': 'geometry',
              'columns': {
                  'geometry': {
                      'encoding': 'WKB',
                      'geometry_types': ['Point']
                  }
              }
          }
          
          import json
          metadata = table.schema.metadata or {}
          metadata[b'geo'] = json.dumps(geo_metadata).encode('utf-8')
          table = table.replace_schema_metadata(metadata)
          
          # Write and read back
          with tempfile.TemporaryDirectory() as tmpdir:
              test_file = Path(tmpdir) / 'test.parquet'
              pq.write_table(table, test_file)
              
              # Read back
              result = pq.read_table(test_file)
              assert len(result) == 3
              assert b'geo' in result.schema.metadata
              print('✓ Basic read/write test passed')
          "

  test-pyarrow-conda:
    name: PyArrow ${{ matrix.pyarrow-version }} - Python ${{ matrix.python-version }} - ${{ matrix.os }} (conda)
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.10', '3.11']
        pyarrow-version: ['21.0.0', '22.0.0', '23.0.0']
    
    steps:
      - uses: actions/checkout@v6

      - name: Setup Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          python-version: ${{ matrix.python-version }}
          miniforge-version: latest
          use-mamba: true

      - name: Install pyarrow and dependencies via conda
        shell: bash -el {0}
        run: |
          mamba install -y -c conda-forge python=${{ matrix.python-version }} pyarrow=${{ matrix.pyarrow-version }} pytest

      - name: Install package with test dependencies
        shell: bash -el {0}
        run: |
          pip install -e .[dev]

      - name: Verify pyarrow version
        shell: bash -el {0}
        run: |
          python -c "import pyarrow; print(f'PyArrow version: {pyarrow.__version__}')"

      - name: Run pytest
        shell: bash -el {0}
        run: |
          pytest -v --tb=short

      - name: Run basic smoke test (read/write GeoParquet)
        shell: bash -el {0}
        run: |
          python -c "
          import tempfile
          import os
          import pyarrow as pa
          import pyarrow.parquet as pq
          from pathlib import Path
          
          # Create a simple test parquet file with geo metadata
          table = pa.table({
              'id': [1, 2, 3],
              'geometry': [b'POINT(0 0)', b'POINT(1 1)', b'POINT(2 2)']
          })
          
          # Add minimal GeoParquet metadata
          geo_metadata = {
              'version': '1.0.0',
              'primary_column': 'geometry',
              'columns': {
                  'geometry': {
                      'encoding': 'WKB',
                      'geometry_types': ['Point']
                  }
              }
          }
          
          import json
          metadata = table.schema.metadata or {}
          metadata[b'geo'] = json.dumps(geo_metadata).encode('utf-8')
          table = table.replace_schema_metadata(metadata)
          
          # Write and read back
          with tempfile.TemporaryDirectory() as tmpdir:
              test_file = Path(tmpdir) / 'test.parquet'
              pq.write_table(table, test_file)
              
              # Read back
              result = pq.read_table(test_file)
              assert len(result) == 3
              assert b'geo' in result.schema.metadata
              print('✓ Basic read/write test passed')
          "
